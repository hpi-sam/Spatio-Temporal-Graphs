{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GraphMatchingAutoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch networkx numpy tqdm sklearn matplotlib wandb"
      ],
      "metadata": {
        "id": "FsaPS5DOcwhz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cpu.html"
      ],
      "metadata": {
        "id": "FJmnQmji6SF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GhrOIuVR3NQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "wV7XhmrqODnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1HgszFDAclEk"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.utils.convert import to_networkx\n",
        "import math\n",
        "import gzip\n",
        "import copy\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "def decode_temporal_graphs(encoded_graphs):\n",
        "    \"\"\"\n",
        "    Decodes a list of graphs from a array.\n",
        "    \"\"\"\n",
        "    graph_seperator = 255\n",
        "    edge_separator=254\n",
        "    graphs = []\n",
        "    current_graph = nx.DiGraph()\n",
        "    edge_start = None\n",
        "    edge_end = None\n",
        "    edge_timestep = None\n",
        "    for number in encoded_graphs:\n",
        "        if number == graph_seperator:\n",
        "            if edge_start is not None or edge_end is not None or edge_timestep is not None:\n",
        "                raise ValueError(\"Invalid encoded graph\")\n",
        "            graphs.append(current_graph)\n",
        "            current_graph = nx.DiGraph()\n",
        "        else:\n",
        "            if edge_start is None:\n",
        "                edge_start = number\n",
        "                continue\n",
        "            if edge_end is None:\n",
        "                edge_end = number\n",
        "                continue\n",
        "            if edge_timestep is None:\n",
        "                edge_timestep = number\n",
        "                continue\n",
        "            if number == edge_separator:\n",
        "                if edge_start is None or edge_end is None or edge_timestep is None:\n",
        "                    raise ValueError(\"Invalid encoded graph\")\n",
        "                current_graph.add_edge(edge_start, edge_end, timestep=edge_timestep)\n",
        "                edge_start = None\n",
        "                edge_end = None\n",
        "                edge_timestep = None\n",
        "                continue\n",
        "            raise ValueError(\"Invalid encoded graph\")\n",
        "    return graphs\n",
        "\n",
        "def load_target_graphs(path):\n",
        "    _graphs= np.load(path, allow_pickle=True)\n",
        "    graphs = []\n",
        "    timesteps = []\n",
        "    not_connected = 0\n",
        "    for g in _graphs:\n",
        "      current_graph = nx.DiGraph()\n",
        "      current_graph.add_nodes_from(list(range(len(g))))\n",
        "      for i, row in enumerate(g):\n",
        "        for j, edge_enc in enumerate(row):\n",
        "          timestep = np.where(edge_enc == 1)[0]\n",
        "          if len(timestep) > 0:\n",
        "            current_graph.add_edge(i, j, timestep=int(timestep[0]))\n",
        "      if len(current_graph.edges()) > 0:\n",
        "        graphs.append(current_graph)\n",
        "      else:\n",
        "        #not_connected += 1\n",
        "        # NOTE: quick-fix to handle disconnected graph\n",
        "        current_graph.add_edge(0, 1, timestep=1)\n",
        "        graphs.append(current_graph)\n",
        "\n",
        "    print(f\"{not_connected} or {(not_connected/ len(_graphs)) * 100}% of graphs have been filtered as they have no edges\" )\n",
        "    return graphs\n",
        "\n",
        "def load_target_graphs_dict(path):\n",
        "    graph_dict = np.load(path, allow_pickle=True)\n",
        "    keys = list(graph_dict.keys())[:1000]\n",
        "    _graphs = [graph_dict[key] for key in keys]\n",
        "    graphs = []\n",
        "    timesteps = []\n",
        "    not_connected = 0\n",
        "    for g in _graphs:\n",
        "      current_graph = nx.DiGraph()\n",
        "      current_graph.add_nodes_from(list(range(len(g))))\n",
        "      for i, row in enumerate(g):\n",
        "        for j, edge_enc in enumerate(row):\n",
        "          timestep = np.where(edge_enc == 1)[0]\n",
        "          if len(timestep) > 0:\n",
        "            current_graph.add_edge(i, j, timestep=int(timestep[0]))\n",
        "      if len(current_graph.edges()) > 0:\n",
        "        graphs.append(current_graph)\n",
        "      else:\n",
        "        not_connected += 1\n",
        "    print(f\"{(not_connected/ len(keys)) * 100}% of graphs filtered as they have no edges\" )\n",
        "    return graphs\n",
        "\n",
        "\n",
        "def load_source_graphs(path):\n",
        "    encoded_graphs = np.load(path)\n",
        "    graphs = decode_temporal_graphs(encoded_graphs)\n",
        "#    for g in graphs:\n",
        "#      timesteps = nx.get_edge_attributes(g, 'timestep')\n",
        "#      edges = copy.deepcopy(g.edges())\n",
        "#      for e in edges:\n",
        "#        g.add_edge(e[1], e[0], timestep=timesteps[e])\n",
        "    return graphs\n",
        "\n",
        "def create_mlp(dims, dropout = None):\n",
        "    layers = [] if dropout is None else [nn.Dropout(dropout)]\n",
        "    for i, dim in enumerate(dims[:-1]):\n",
        "      layers.append(nn.Linear(dim, dims[i+1]))\n",
        "      if i < len(dims[:-1]) - 1:\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def perform_conv(convs, act, emb, edge_index, edge_feats = None):\n",
        "    for i, conv in enumerate(convs):\n",
        "      emb = conv(emb, edge_index, edge_feats)\n",
        "      if i < len(convs) - 1:\n",
        "        emb = act(emb)\n",
        "    return emb\n",
        "\n",
        "class MetricsLogger():\n",
        "  def __init__(self):\n",
        "    self.reset()\n",
        "    self._best_acc = 0\n",
        "    self._best_acc_epoch = 0\n",
        "\n",
        "  def log_loss(self, loss):\n",
        "    self._loss += loss\n",
        "    self._step += 1\n",
        "  \n",
        "  def log_prediction(self, preds, labels, num_classes=None):\n",
        "    # no classification\n",
        "    if num_classes is None:\n",
        "      self._preds += preds.tolist()\n",
        "      self._labels += labels.tolist() if len(labels) > 0 else []\n",
        "    # binary classification w/ multi-labels\n",
        "    elif num_classes > 1:\n",
        "      self._preds += np.argmax(preds, axis=-1).tolist()\n",
        "      self._labels += np.argmax(labels, axis=-1).tolist()\n",
        "    # binary classification\n",
        "    else:\n",
        "      self._labels += labels.tolist()\n",
        "      self._preds += (preds > 0).tolist()\n",
        "  \n",
        "  def reset(self):\n",
        "    self._loss = 0\n",
        "    self._step = 0\n",
        "    self._labels = []\n",
        "    self._embeddings = []\n",
        "    self._preds = []\n",
        "    self._labels = []\n",
        "\n",
        "  def get_embeddings(self):\n",
        "    return np.array(self._preds)\n",
        "    \n",
        "  def get_loss(self):\n",
        "    loss = math.floor((self._loss / self._step) * 10000) / 10000\n",
        "    return loss\n",
        "\n",
        "  def log_best_acc(self, acc, epoch):\n",
        "    if acc > self._best_acc:\n",
        "      self._best_acc = acc\n",
        "      self._best_acc_epoch = epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric.data as geom_data\n",
        "from torch_geometric.nn import TransformerConv, global_mean_pool, GATConv, GatedGraphConv, GINEConv, SplineConv\n",
        "from torch_geometric.utils import to_dense_adj, from_networkx\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch import optim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import os\n",
        "from torch_geometric.data import InMemoryDataset, download_url, DataLoader\n",
        "from torch_geometric.datasets import TUDataset\n",
        "import networkx as nx\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed + 1)\n",
        "np.random.seed(seed + 2)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/graph_matching/\"\n",
        "\n",
        "class TUGraphDataset():\n",
        "  def __init__(self, name, batch_size):\n",
        "    print(f\"Load {name} dataset\")\n",
        "    dataset = TUDataset(root=f\"/tmp/{name.upper()}\", name=name.upper(), use_edge_attr=True)\n",
        "    dataset = dataset.shuffle()\n",
        "    split = int(len(dataset) * 0.8)\n",
        "    train_dataset = dataset[:split]\n",
        "    test_dataset = dataset[split:]\n",
        "\n",
        "    self.adj_dim = max([data.x.shape[0] for data in dataset])\n",
        "    pos = sum([data.y[0] for data in dataset])\n",
        "    self.node_feat_dim = dataset.num_node_features\n",
        "    self.edge_feat_dim = dataset.num_edge_features\n",
        "    self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    self.unshuffled_train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "    self.test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "    self.num_classes = dataset.num_classes\n",
        "\n",
        "    print(f\"Ratio is {pos/len(dataset)} and max nodes are {self.adj_dim}\")\n",
        "\n",
        "class AnomalyDataset(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
        "        super().__init__(root, transform, pre_transform, pre_filter)\n",
        "        collate, self.split = torch.load(self.processed_paths[0])\n",
        "        self.data, self.slices = collate\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return []\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['processed_anomaly_graphs.pt']\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        source_path = PATH + 'apg_unique_subgraphs_temporal_node2-17.npy'\n",
        "        target_path = PATH + '100k_frontend_graphs_temporal_unique.npy'\n",
        "        source_graphs = load_source_graphs(source_path)\n",
        "        target_graphs = load_target_graphs(target_path)\n",
        "        print(f\"Found {len(source_graphs)} source graphs and {len(target_graphs)} target graphs\")\n",
        "        graphs = source_graphs + target_graphs\n",
        "        data = []\n",
        "        split = len(source_graphs)\n",
        "        for graph in tqdm(graphs, 'Transform graphs'):\n",
        "          # TODO: try to set node label as attributes\n",
        "          nx.set_node_attributes(graph, [1., 1., 1., 1., 1., 1., 1., 1.], \"labels\")\n",
        "          edge_labels = nx.get_edge_attributes(graph, 'timestep')\n",
        "          new_edge_labels = {}\n",
        "          for k, v in edge_labels.items():\n",
        "            new_edge_labels[k] = float(v)\n",
        "          nx.set_edge_attributes(graph, new_edge_labels, 'timestep')\n",
        "          d = from_networkx(graph, group_node_attrs=['labels'], group_edge_attrs=['timestep'])\n",
        "          data.append(d)\n",
        "\n",
        "        torch.save((self.collate(data), split), self.processed_paths[0])\n",
        "\n",
        "class TGAnomalyDataset():\n",
        "    \"\"\"Graph anomaly dataset.\"\"\"\n",
        "    def __init__(self, batch_size):  \n",
        "        print(\"Load anomaly dataset\")      \n",
        "        dataset = AnomalyDataset(root=f\"/tmp/anomaly\")\n",
        "        split = dataset.split\n",
        "        test_dataset = dataset[:split]\n",
        "        train_dataset = dataset[split:]\n",
        "\n",
        "        self.adj_dim = max([data.x.shape[0] for data in dataset])\n",
        "        self.node_feat_dim = dataset.num_node_features\n",
        "        self.edge_feat_dim = dataset.num_edge_features\n",
        "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        self.unshuffled_train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "        self.test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "        self.num_classes = None\n",
        "        self.source_graphs = [to_networkx(data) for data in test_dataset]\n",
        "        self.target_graphs = [to_networkx(data) for data in train_dataset]\n",
        "\n",
        "class TGDecoder(nn.Module):\n",
        "    def __init__(self, adj_dim, node_feat_dim, edge_feat_dim, hidden_dims, emb_dim, dp):\n",
        "        super(TGDecoder, self).__init__()\n",
        "        self.adj_dim = adj_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.edge_feat_dim = edge_feat_dim if edge_feat_dim > 0 else None\n",
        "\n",
        "        self.edge_mlp = create_mlp([adj_dim, adj_dim * int(self.edge_feat_dim / 2), adj_dim *  self.edge_feat_dim] if \n",
        "                                     self.edge_feat_dim is not None and self.edge_feat_dim > 2 else [adj_dim, adj_dim, adj_dim])\n",
        "        self.graph_mlp = create_mlp([emb_dim, *hidden_dims, emb_dim * adj_dim], dp)\n",
        "\n",
        "    def forward(self, emb):\n",
        "        batch_size = emb.shape[0]\n",
        "        emb = self.graph_mlp(emb)\n",
        "        emb = torch.reshape(emb, (batch_size, self.adj_dim, self.emb_dim))\n",
        "        adj = torch.bmm(emb, torch.transpose(emb, 1, 2))\n",
        "        if self.edge_feat_dim is not None:\n",
        "          adj = self.edge_mlp(adj)\n",
        "          adj = torch.reshape(adj, (batch_size, self.adj_dim, self.adj_dim, self.edge_feat_dim)) \n",
        "        return adj\n",
        "\n",
        "class TGEncoder(nn.Module):\n",
        "    def __init__(self, node_feat_dim, edge_feat_dim, conv_dims, emb_dim, attention_heads, linear_dp, conv_dp, gnn_layer):\n",
        "        super(TGEncoder, self).__init__()\n",
        "        conv_dims = [node_feat_dim, *conv_dims]\n",
        "        self.edge_feat_dim = edge_feat_dim if edge_feat_dim > 0 else None\n",
        "        self.gnn_layer = gnn_layer\n",
        "        if gnn_layer == \"TransformerConv\":\n",
        "          self.convs = nn.ParameterList([TransformerConv(dim, conv_dims[i+1], \n",
        "                                            edge_dim=self.edge_feat_dim, heads=attention_heads, \n",
        "                                            concat=attention_heads == 1, beta=False, dropout=conv_dp) for i, dim in enumerate(conv_dims[:-1])])\n",
        "        elif gnn_layer == \"GATConv\":\n",
        "          self.convs = nn.ParameterList([GATConv(dim, conv_dims[i+1], \n",
        "                                            edge_dim=self.edge_feat_dim, heads=attention_heads, \n",
        "                                            concat=attention_heads == 1, dropout=conv_dp) for i, dim in enumerate(conv_dims[:-1])])\n",
        "        elif gnn_layer == \"GINEConv\":\n",
        "          self.convs = nn.ParameterList([GINEConv(nn=nn.Sequential(*[nn.Linear(dim, conv_dims[i+1])]), \n",
        "                                            edge_dim=edge_feat_dim if edge_feat_dim > 0 else 1) for i, dim in enumerate(conv_dims[:-1])])\n",
        "        else:\n",
        "          raise RuntimeError('Given gnn layer not defined')\n",
        "\n",
        "        self.act = nn.ReLU()\n",
        "        self.emb_mlp = create_mlp([conv_dims[-1], emb_dim], linear_dp)\n",
        "\n",
        "    def forward(self, node_feats, edge_index, batch_idx, edge_feats):\n",
        "        if self.gnn_layer == \"GINEConv\" and edge_feats is None:\n",
        "          edge_feats = torch.tensor(np.ones((edge_index.shape[1], 1))).float()\n",
        "        emb = perform_conv(self.convs, self.act, node_feats, edge_index, edge_feats)\n",
        "        emb = global_mean_pool(emb, batch_idx)\n",
        "        emb = self.emb_mlp(emb)\n",
        "        return emb\n",
        "\n",
        "class TGAutoEncoder(nn.Module):\n",
        "    def __init__(self, adj_dim, node_feat_dim, edge_feat_dim, emb_dim, enc_hidden_dims, dec_hidden_dims, attention_heads, \n",
        "                 linear_dp, conv_dp, gnn_layer):\n",
        "        super(TGAutoEncoder, self).__init__()\n",
        "        self.encoder = TGEncoder(node_feat_dim, edge_feat_dim, enc_hidden_dims, emb_dim, attention_heads, linear_dp, conv_dp, gnn_layer)\n",
        "        self.decoder = TGDecoder(adj_dim, node_feat_dim, edge_feat_dim, dec_hidden_dims, emb_dim, linear_dp)\n",
        "\n",
        "    def forward(self, node_feats, edge_index, batch_idx, edge_feats):\n",
        "        enc = self.encoder(node_feats, edge_index, batch_idx, edge_feats)\n",
        "        dec_adj = self.decoder(enc)\n",
        "        return enc, dec_adj\n",
        "\n",
        "    def classify(self, train_preds, train_labels, eval_preds, eval_labels):\n",
        "      train_acc = accuracy_score(train_labels, train_preds)\n",
        "      test_acc = accuracy_score(eval_labels, eval_preds)\n",
        "      return math.floor(train_acc * 10000) / 100, math.floor(test_acc * 10000) / 100\n",
        "\n",
        "    def classify_emb(self, train_embeddings, train_labels, eval_embeddings, eval_labels):\n",
        "      clf = LogisticRegression(max_iter=2000)\n",
        "      clf.fit(train_embeddings, train_labels)\n",
        "      return math.floor(clf.score(train_embeddings, train_labels) * 10000) / 100, math.floor(clf.score(eval_embeddings, eval_labels) * 10000) / 100\n",
        "\n",
        "def get_dataset(name, batch_size):\n",
        "  if name == \"anomaly\":\n",
        "    return TGAnomalyDataset(batch_size=batch_size)\n",
        "  if name == \"proteins\":\n",
        "    return TUGraphDataset(name=\"proteins\", batch_size=batch_size)\n",
        "  if name == \"enzymes\":\n",
        "    return TUGraphDataset(name=\"enzymes\", batch_size=batch_size)\n",
        "  if name == \"mutag\":\n",
        "    return TUGraphDataset(name=\"mutag\", batch_size=batch_size)\n",
        "  if name == \"bzr\":\n",
        "    return TUGraphDataset(name=\"bzr_md\", batch_size=batch_size)\n",
        "\n",
        "batch_size = 64\n",
        "dataset_name = \"anomaly\"\n",
        "dataset = get_dataset(dataset_name, batch_size)\n",
        "criterion = nn.MSELoss()\n",
        "CLASSIFICATION = \"log\" # \"linear\", \"log\", None\n",
        "emb_dim = 16\n",
        "lr = 1e-4\n",
        "num_epochs = 5\n",
        "enc_hidden_dims = [128, 128, 128]\n",
        "dec_hidden_dims = [32, 64, 128]\n",
        "attention_heads = 1\n",
        "linear_dp = 0.6\n",
        "conv_dp = 0.\n",
        "gnn_layer = \"TransformerConv\" # \"TransformerConv\", \"GATConv\", \"GINEConv\"\n",
        "LOG = False\n",
        "\n",
        "if dataset_name == \"anomaly\":\n",
        "  emb_dim = 17\n",
        "if dataset.num_classes is None:\n",
        "  CLASSIFICATION = None\n",
        "if CLASSIFICATION == \"linear\":\n",
        "  emb_dim = 1 if dataset.num_classes==2 else dataset.num_classes\n",
        "  criterion = nn.BCEWithLogitsLoss() if dataset.num_classes==2 else nn.CrossEntropyLoss()\n",
        "\n",
        "model = TGAutoEncoder(adj_dim=dataset.adj_dim, node_feat_dim=dataset.node_feat_dim, edge_feat_dim=dataset.edge_feat_dim, emb_dim=emb_dim, \n",
        "                      enc_hidden_dims=enc_hidden_dims, dec_hidden_dims=dec_hidden_dims, attention_heads=attention_heads, linear_dp=linear_dp, \n",
        "                      conv_dp=conv_dp, gnn_layer=gnn_layer)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "if LOG:\n",
        "  wandb.init(project=\"graph-matching-autoencoder\", entity=\"schti\", tags=[dataset_name])\n",
        "  wandb.config.update({\n",
        "    \"dataset\": dataset_name,\n",
        "    \"classification_type\": CLASSIFICATION,\n",
        "    \"learning_rate\": lr,\n",
        "    \"epochs\": num_epochs,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"encoder_hidden_dims\": enc_hidden_dims,\n",
        "    \"decoder_hidden_dims\": dec_hidden_dims,\n",
        "    \"embedding_dim\": emb_dim,\n",
        "    \"attention_heads\": attention_heads,\n",
        "    \"linear_dropout\": linear_dp,\n",
        "    \"conv_dropout\": conv_dp,\n",
        "    \"gnn_layer\": gnn_layer,\n",
        "  })\n",
        "  wandb.run.name = f\"run_{dataset_name}_{num_epochs}\"\n",
        "  wandb.define_metric(\"train_step\")\n",
        "  wandb.define_metric(\"train_loss\", step_metric=\"train_step\")\n",
        "  wandb.define_metric(\"eval_step\")\n",
        "  wandb.define_metric(\"eval_loss\", step_metric=\"eval_step\")\n",
        "\n",
        "print(f\"Start training for {num_epochs} epochs\")\n",
        "train_logger = MetricsLogger()\n",
        "eval_logger = MetricsLogger()\n",
        "train_step = 0\n",
        "eval_step = 0\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  train_logger.reset()\n",
        "  for data in tqdm(dataset.train_loader, f\"Epoch {epoch +1}\"):\n",
        "    node_feats, edge_index, batch_idx, edge_feats, labels = data.x, data.edge_index, data.batch, data.edge_attr, data.y\n",
        "    optimizer.zero_grad()\n",
        "    preds, adj = model(node_feats, edge_index, batch_idx, edge_feats)\n",
        "    o_adj = to_dense_adj(edge_index=edge_index, batch=batch_idx, edge_attr=edge_feats, max_num_nodes=dataset.adj_dim)\n",
        "    if CLASSIFICATION == \"linear\":\n",
        "      if emb_dim == 1:\n",
        "        preds = preds.squeeze(dim=-1)\n",
        "      else:\n",
        "        labels = F.one_hot(labels).float()\n",
        "      preds = preds.float()\n",
        "      labels = labels.float()\n",
        "      preds = preds.squeeze(dim=-1)\n",
        "      loss = criterion(preds, labels)\n",
        "    else:\n",
        "      loss = criterion(adj, o_adj)\n",
        "    if LOG:\n",
        "      wandb.log({'train_loss': loss.item(), \"train_step\": train_step})\n",
        "    train_logger.log_loss(loss.item())\n",
        "    train_logger.log_prediction(preds.detach().numpy(), [] if labels is None else labels.detach().numpy(), num_classes=emb_dim if CLASSIFICATION == \"linear\" else None)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_step+=1\n",
        "\n",
        "  model.eval()\n",
        "  eval_logger.reset()\n",
        "  for data in dataset.test_loader:\n",
        "    node_feats, edge_index, batch_idx, edge_feats, labels = data.x, data.edge_index, data.batch, data.edge_attr, data.y\n",
        "    preds, adj = model(node_feats, edge_index, batch_idx, edge_feats)\n",
        "    o_adj = to_dense_adj(edge_index=edge_index, batch=batch_idx, edge_attr=edge_feats, max_num_nodes=dataset.adj_dim)\n",
        "    \n",
        "    if CLASSIFICATION == \"linear\":\n",
        "      if emb_dim == 1:\n",
        "        labels = labels.float()\n",
        "      else:\n",
        "        labels = F.one_hot(labels).float()\n",
        "      preds = preds.float()\n",
        "      preds = preds.squeeze(dim=-1)\n",
        "      loss = criterion(preds, labels)\n",
        "    else:\n",
        "      loss = criterion(adj, o_adj)\n",
        "    if LOG:\n",
        "      wandb.log({'eval_loss': loss.item(), \"eval_step\": eval_step})\n",
        "    eval_logger.log_loss(loss.item())\n",
        "    eval_logger.log_prediction(preds.detach().numpy(), [] if labels is None else labels.detach().numpy(), num_classes=emb_dim if CLASSIFICATION == \"linear\" else None)\n",
        "    eval_step+=1\n",
        "\n",
        "  train_loss = train_logger.get_loss()\n",
        "  eval_loss = eval_logger.get_loss()\n",
        "\n",
        "  results = {}\n",
        "  if CLASSIFICATION is not None and CLASSIFICATION == \"linear\":\n",
        "    train_acc, eval_acc = model.classify(train_logger._preds, train_logger._labels, eval_logger._preds, eval_logger._labels)\n",
        "    results = {'train_acc': train_acc, 'eval_acc': eval_acc}\n",
        "    eval_logger.log_best_acc(eval_acc, epoch)\n",
        "  elif CLASSIFICATION is not None and CLASSIFICATION == \"log\":\n",
        "    train_acc, eval_acc = model.classify_emb(train_logger._preds, train_logger._labels, eval_logger._preds, eval_logger._labels)\n",
        "    results =  {'train_acc': train_acc, 'eval_acc': eval_acc}\n",
        "    eval_logger.log_best_acc(eval_acc, epoch)\n",
        "  \n",
        "  result = \"\".join([f\"{key}: {value}\" for key, value in results.items()])\n",
        "  print(f\"Epoch {epoch + 1}/{num_epochs} - train_loss: {train_loss} - eval_loss: {eval_loss}\" + (f\" && {result}\" if len(result) > 0 else ''))\n",
        "\n",
        "if CLASSIFICATION is not None:\n",
        "  print(f\"Best acc was {eval_logger._best_acc} after {eval_logger._best_acc_epoch +1} epochs!\")\n",
        "\n",
        "def save_embeddings(embeddings, file):\n",
        "  with open(f'{PATH}{file}.npy', 'wb') as f:\n",
        "    np.save(f, embeddings)\n",
        "\n",
        "def load_embeddings(file):\n",
        "  with open(f'{PATH}{file}.npy', 'rb') as f:\n",
        "    return np.load(f)\n",
        "\n",
        "name = f\"embeddings_{emb_dim}_dims_{num_epochs}_epochs\"\n",
        "target_embeddings = np.array([])\n",
        "source_embeddings = np.array([])\n",
        "model.eval()\n",
        "for data in tqdm(dataset.unshuffled_train_loader, f\"Save target embeddings\"):\n",
        "    node_feats, edge_index, batch_idx, edge_feats, labels = data.x, data.edge_index, data.batch, data.edge_attr, data.y\n",
        "    emb, _ = model(node_feats, edge_index, batch_idx, edge_feats)\n",
        "    target_embeddings = np.concatenate((target_embeddings, emb.detach().numpy())) if len(target_embeddings) > 0 else emb.detach().numpy()\n",
        "for data in tqdm(dataset.test_loader, f\"Save source embeddings\"):\n",
        "    node_feats, edge_index, batch_idx, edge_feats, labels = data.x, data.edge_index, data.batch, data.edge_attr, data.y\n",
        "    emb, _ = model(node_feats, edge_index, batch_idx, edge_feats)\n",
        "    source_embeddings = np.concatenate((source_embeddings, emb.detach().numpy())) if len(source_embeddings) > 0 else emb.detach().numpy()\n",
        "save_embeddings(target_embeddings, dataset_name + \"_target_\" + name)\n",
        "save_embeddings(source_embeddings, dataset_name + \"_source_\" + name)\n",
        "print(\"Embeddings saved!\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "rankings = []\n",
        "for source in eval_logger.get_embeddings():\n",
        "  distances = []\n",
        "  for target in train_logger.get_embeddings():\n",
        "    dist = np.linalg.norm(target-source)\n",
        "    distances.append(dist)\n",
        "  rankings.append(np.argsort(distances))\n",
        "\n",
        "s_idx = 3\n",
        "t_idxs = range(5)\n",
        "s = [(dataset.source_graphs[s_idx], \"source\")]\n",
        "t = [(dataset.target_graphs[rankings[s_idx][t_idx]], f\"target_{t_idx + 1}\" ) for t_idx in t_idxs]\n",
        "#pos = nx.spring_layout(dataset.source_graphs[s_idx])\n",
        "for i, (g, name) in enumerate([*s, *t]):\n",
        "  pos = nx.spring_layout(g)\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = plt.subplot()\n",
        "  ax.set_title(name)\n",
        "  nx.draw(g, pos)\n",
        "  nx.draw_networkx_labels(g, pos)\n",
        "  edge_labels = nx.get_edge_attributes(g, 'timestep')\n",
        "  nx.draw_networkx_edge_labels(g, pos, edge_labels)\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(f\"/content/drive/MyDrive/graph_matching/{name}.png\", format=\"PNG\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "b2mkWYnx9_e7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}